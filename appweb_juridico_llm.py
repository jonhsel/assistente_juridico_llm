# -*- coding: utf-8 -*-
"""appweb_juridico_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HsZ2-OeH13sjRsqufQH2NxUUthPbNeb_

<!-- Projeto Desenvolvido por Jonh Selmo - Engenheiro de IA -->
# <font color='blue'>Jonh Selmo - Engenheiro de IA</font>
## <font color='blue'>IA Generativa e LLMs Para Processamento de Linguagem Natural</font>
## <font color='blue'>Projeto </font>
## <font color='blue'>Fine-Tuning de LLM Open-Source Para App de Assistente Jurídico</font>

## Instalando e Carregando Pacotes
"""

# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:
# pip install -U nome_pacote

# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:
# !pip install nome_pacote==versão_desejada

# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.

# Instala o pacote watermark.
# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.
!pip install -q -U watermark

# Instalando os pacotes
!pip install -q -r requirements.txt

# Imports
import numpy as np
import nltk
import shutil
import evaluate
import transformers
import datasets
from datasets import load_dataset
from transformers import T5Tokenizer, DataCollatorForSeq2Seq
from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
# Versões dos pacotes usados neste jupyter notebook
# %reload_ext watermark
# %watermark -a "Jonh Selmo - Engenheiro de IA"

# Remove as pastas, se já existirem. Isso evita erros ao executar o jupyter a partir da segunda vez em diante
try:
    shutil.rmtree('logs_treino')
    shutil.rmtree('resultados_treino')
    shutil.rmtree('modelo_salvo')
except:
    print('As pastas não existem ou já foram removidas!')

"""## Carregando os Dados"""

# Define o nome do arquivo
nome_arquivo = "dataset.csv"

# Carrega os dados
dsa_dataset = load_dataset('csv', data_files = nome_arquivo, delimiter = ',')

# Divisão em treino e teste com proporção 80/20
dsa_dataset = dsa_dataset["train"].train_test_split(test_size = 0.2)

# Visualiza o formato do dataset
dsa_dataset

"""## Tokenizador e LLM Open-Source

https://huggingface.co/google/flan-t5-base
"""

# Carrega o tokenizador
tokenizador = T5Tokenizer.from_pretrained("google/flan-t5-base", legacy = False)

# Visualiza o tokenizador
tokenizador

# Carrega o LLM pré-treinado
modelo = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base")

# Visualiza a arquitetura do LLM
modelo

# Data collator para concatenar tokenizador e modelo
data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizador, model = modelo)

# Visualiza o data collator
data_collator

"""## Pré-Processamento"""

# Todos os inputs vão receber como prefixo: "answer the question" (responda a questão)
prefix = "answer the question: "

# Função de pré-processamento
def js_fn_preprocess(data):

    # Concatena o prefixo a cada pergunta na lista de perguntas fornecida em data["question"]
    inputs = [prefix + doc for doc in data["question"]]

    # Usa o tokenizer para converter as perguntas processadas em tokens com um comprimento máximo de 128,
    # truncando as que forem mais longas
    model_inputs = tokenizador(inputs, max_length = 128, truncation = True)

    # Tokeniza as respostas fornecidas em data["answer"] com um comprimento máximo de 512,
    # truncando as que forem mais longas
    labels = tokenizador(text_target = data["answer"], max_length = 512, truncation = True)

    # Adiciona os tokens das respostas como rótulos no dicionário de entradas do modelo
    model_inputs["labels"] = labels["input_ids"]

    # Retorna o dicionário processado contendo tanto as entradas tokenizadas quanto os rótulos
    return model_inputs

# Aplica a função de pré-processamento ao dataset gerando o dataset tokenizado
js_dataset_tokenized = dsa_dataset.map(js_fn_preprocess, batched = True)

# Visualiza o dataset tokenizado
js_dataset_tokenized

js_dataset_tokenized['train']['question'][0]

js_dataset_tokenized['train']['answer'][0]

js_dataset_tokenized['train']['input_ids'][0]

js_dataset_tokenized['train']['labels'][0]

"""## Definindo a Métrica de Avaliação do Modelo"""

# O pacote "punkt" é específico para a tarefa de tokenização, que envolve a divisão de um texto
# em uma lista de sentenças
nltk.download("punkt", quiet = True)

# Download do módulo complementar punkt_tab
nltk.download('punkt_tab')

# Define a métrica
metric = evaluate.load("rouge")

# Função de cálculo das métricas
def js_calcula_metricas(eval_preds):

    # Desempacota as predições e os rótulos do argumento eval_preds
    previsoes, labels = eval_preds

    # Substitui todos os valores diferentes de -100 em labels pelo ID do token de preenchimento
    labels = np.where(labels != -100,
                      labels,
                      tokenizador.pad_token_id)

    # Decodifica as previsões para texto, ignorando tokens especiais
    previsoes_decodificadas = tokenizador.batch_decode(previsoes,
                                                       skip_special_tokens = True)

    # Decodifica os labels para texto, ignorando tokens especiais
    labels_decodificados = tokenizador.batch_decode(labels,
                                                    skip_special_tokens = True)

    # Adiciona uma nova linha após cada sentença para as previsões decodificadas,
    # preparando-as para a avaliação ROUGE
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in previsoes_decodificadas]

    # Adiciona uma nova linha após cada label para as previsões decodificadas,
    # preparando-os para a avaliação ROUGE
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in labels_decodificados]

    # Calcula a métrica ROUGE entre as previções e os rótulos decodificados, utilizando um stemmer
    resultado = metric.compute(predictions = previsoes_decodificadas,
                               references = labels_decodificados,
                               use_stemmer = True)

    # Retorna o resultado da métrica ROUGE
    return resultado

# Define argumentos de treino
js_training_args = Seq2SeqTrainingArguments(output_dir = "resultados_treino",
                                             eval_strategy = "epoch",
                                             learning_rate = 3e-4,
                                             logging_dir = "logs_treino",
                                             logging_steps = 1,
                                             per_device_train_batch_size = 4,
                                             per_device_eval_batch_size = 2,
                                             weight_decay = 0.01,
                                             save_total_limit = 3,
                                             num_train_epochs = 6,
                                             predict_with_generate = True,
                                             push_to_hub = False,
                                             report_to = "none")

# Define o trainer
js_trainer = Seq2SeqTrainer(model = modelo,
                             args = js_training_args,
                             train_dataset = js_dataset_tokenized["train"],
                             eval_dataset = js_dataset_tokenized["test"],
                             tokenizer = tokenizador,
                             data_collator = data_collator,
                             compute_metrics = js_calcula_metricas)

"""## Treinamento do Modelo"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# js_trainer.train()

# Salva o modelo
js_trainer.save_model('modelo_salvo')

"""**ROUGE-1** mede a sobreposição de unigramas (palavras individuais).

**ROUGE-2** mede a sobreposição de bigramas (pares de palavras consecutivas).

**ROUGE-L** mede a sobreposição da subsequência mais longa comum entre o resumo gerado e o resumo de referência. Isso leva em conta a ordem das palavras, mas permite lacunas. Valores mais altos indicam melhor desempenho. ROUGE-L é calculado com base na similaridade entre as sequências, levando em consideração a precisão, o recall e a média harmônica entre eles.

Valores mais altos de ROUGE indicam uma maior semelhança entre o resumo gerado e o resumo de referência, o que geralmente é interpretado como uma indicação de melhor qualidade do resumo. No entanto, é importante lembrar que nenhuma métrica pode capturar completamente a qualidade de um resumo e é útil complementar a avaliação com análises qualitativas ou outras métricas.

## Deploy e Uso do Modelo
"""

# Carrega o tokenizador final salvo em disco
tokenizador_js_final = AutoTokenizer.from_pretrained('modelo_salvo')

# Carrega o modelo final salvo em disco
modelo_js_final = AutoModelForSeq2SeqLM.from_pretrained('modelo_salvo')

# Visualiza a arquitetura do modelo
modelo_js_final

# Texto de entrada
texto_entrada = "É obrigatória a comunicação do arquivamento ao Juízo competente?"

# Tokeniza o input
texto_entrada_tokenizado = tokenizador_js_final(texto_entrada, return_tensors = "pt").input_ids

# Gera a saída (previsão do modelo)
texto_saida_tokenizado = modelo_js_final.generate(texto_entrada_tokenizado,
                                                   max_length = 50,
                                                   temperature = 0.4,
                                                   do_sample = True)

# Visualiza
texto_saida_tokenizado

# Decode da saída
texto_saida = tokenizador_js_final.decode(texto_saida_tokenizado[0],
                                           skip_special_tokens = True)

# Resultado
print("Pergunta:", texto_entrada)
print("Resposta:", texto_saida)

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext watermark
# %watermark -a "Jonh Selmo - Engenheiro de IA"

#%watermark -v -m

#%watermark --iversions

"""# Fim"""